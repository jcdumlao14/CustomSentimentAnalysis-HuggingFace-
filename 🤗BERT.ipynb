{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzJyzeKQUkE9jzL4XCjgSM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcdumlao14/CustomSentimentAnalysis-HuggingFace-/blob/main/%F0%9F%A4%97BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Sentiment Analysis with Hugging Face ðŸ¤— - BERT**"
      ],
      "metadata": {
        "id": "ywJIO4z9K8Uc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom sentiment analysis with Hugging Face involves using pre-trained transformer models like BERT, RoBERTa, or DistilBERT, fine-tuning them on a custom dataset, and then using the fine-tuned model to predict the sentiment of new text inputs.\n",
        "\n",
        "Hugging Face provides a wide range of pre-trained transformer models that can be fine-tuned for sentiment analysis tasks. The advantage of using Hugging Face is that it provides a simple and powerful Python library called transformers that makes it easy to fine-tune transformer models on custom datasets.\n",
        "\n",
        "Using the transformers library, you can load a pre-trained transformer model, customize its configuration, add a classification head for sentiment analysis, and fine-tune the model on your custom dataset. Once the model is fine-tuned, you can use it to predict the sentiment of new text inputs.\n",
        "\n",
        "Hugging Face also provides a hosted service called Hugging Face Spaces, which allows you to easily deploy your fine-tuned model to the cloud and create an API endpoint for sentiment analysis. This makes it easy to integrate custom sentiment analysis into your applications without having to manage infrastructure or deployment yourself."
      ],
      "metadata": {
        "id": "x04PAVjZK3L-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Install the necessary libraries:**\n",
        "\n",
        "This step involves installing the required Python libraries, which may include Transformers, TensorFlow, and NumPy."
      ],
      "metadata": {
        "id": "05VVq-eJLBvM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us24EZsIK1Z4",
        "outputId": "954e5165-2aff-45a8-d615-cc49b4353b43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "II6JYT1xOLoe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pandas"
      ],
      "metadata": {
        "id": "y5YXUkn0NkYb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Import the necessary libraries and load the dataset:**\n",
        "\n",
        "After installing the required libraries, the next step is to import them into the Python script. The necessary libraries include TensorFlow, Hugging Face transformers, NumPy, and Pandas. The dataset is loaded using the Pandas library, which is used to read the CSV file containing the data.\n"
      ],
      "metadata": {
        "id": "nlnlKU3LLKfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, TextClassificationPipeline\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/sms_spam.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "iaiH1f17LJry",
        "outputId": "33247aed-4ab3-4085-b66b-654bf79dfa7a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   type                                               text\n",
              "0   ham  Hope you are having a good week. Just checking in\n",
              "1   ham                            K..give back my thanks.\n",
              "2   ham        Am also doing in cbe only. But have to pay.\n",
              "3  spam  complimentary 4 STAR Ibiza Holiday or Â£10,000 ...\n",
              "4  spam  okmail: Dear Dave this is your final notice to..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f0e6924-22b8-4b1d-9a89-ca019f5c87cb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Hope you are having a good week. Just checking in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>K..give back my thanks.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>Am also doing in cbe only. But have to pay.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>spam</td>\n",
              "      <td>complimentary 4 STAR Ibiza Holiday or Â£10,000 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>spam</td>\n",
              "      <td>okmail: Dear Dave this is your final notice to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f0e6924-22b8-4b1d-9a89-ca019f5c87cb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f0e6924-22b8-4b1d-9a89-ca019f5c87cb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f0e6924-22b8-4b1d-9a89-ca019f5c87cb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Preprocess the dataset:**\n",
        "\n",
        "The dataset is preprocessed to remove any unwanted characters, symbols, or spaces. The text data is also tokenized and converted into numerical vectors that can be fed into the deep learning model."
      ],
      "metadata": {
        "id": "sY3jDDKZLezO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert ham to 0 and spam to 1\n",
        "df[\"type\"] = df[\"type\"].map({\"ham\": 0, \"spam\": 1})\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(df[\"text\"], df[\"type\"], test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "Kkax15tjLbK6"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Load the pre-trained transformer model:**\n",
        "\n",
        "A pre-trained transformer model is loaded from the Hugging Face transformers library. The transformer model is responsible for learning the relationships between the input text and their corresponding sentiment labels."
      ],
      "metadata": {
        "id": "r5sJOf48LoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCb9w6EvLnsv",
        "outputId": "24c4f5e6-9b78-4e59-ac04-332f4a37aabe"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Fine-tune the model:**\n",
        "\n",
        "The pre-trained transformer model is fine-tuned on the training data to adapt it to the specific sentiment analysis task. This involves adjusting the model's weights through multiple epochs of training."
      ],
      "metadata": {
        "id": "dlm_k5HTL2pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFPreTrainedModel\n",
        "\n",
        "class MyModel(TFPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.bert = TFBertModel(config)\n",
        "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = tf.keras.layers.Dense(config.num_labels, activation=tf.keras.activations.softmax, name=\"classifier\")\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        outputs = self.bert(inputs, **kwargs)\n",
        "        pooled_output = outputs[1]\n",
        "        pooled_output = self.dropout(pooled_output, training=kwargs.get(\"training\", False))\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "    def compute_loss(self, labels, logits):\n",
        "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "        loss = loss_fn(labels, logits)\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "kyMZimghPkvW"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the input data\n",
        "train_texts = [\"This is the first text\", \"This is the second text\"]\n",
        "train_labels = [1, 0]\n",
        "train_encodings = tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "test_texts = [\"This is the third text\", \"This is the fourth text\"]\n",
        "test_labels = [0, 1]\n",
        "test_encodings = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "# Prepare the training and testing datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels)).batch(16)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels)).batch(16)\n",
        "\n",
        "# Define the optimizer and the loss function\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
        "\n",
        "# Fine-tune the model\n",
        "history = model.fit(train_dataset, epochs=3, batch_size=16)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nduwtw0iexEO",
        "outputId": "56ad5e37-97ee-49d6-c0a0-4197e8b688ea"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f41df184dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 43s 43s/step - loss: 0.7006 - accuracy: 0.5000\n",
            "Epoch 2/3\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7260 - accuracy: 0.0000e+00\n",
            "Epoch 3/3\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6927 - accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Evaluate the model:**\n",
        "\n",
        "After fine-tuning the model, it is evaluated on the validation data to check its performance. The evaluation metrics can include accuracy, precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "U6MvrmHEL_tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_dataset, batch_size=16)\n",
        "print(\"Test accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrP8R2hNfNvQ",
        "outputId": "08d4b33c-c0fc-48ae-b948-bec2e9cd73e7"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 7s 7s/step - loss: 0.6852 - accuracy: 1.0000\n",
            "Test accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Predict the sentiment of new text inputs:**\n",
        "\n",
        "Once the model is trained and evaluated, it can be used to predict the sentiment of new text inputs. The new text input is preprocessed in the same way as the training data, and the pre-trained model is used to predict its sentiment label."
      ],
      "metadata": {
        "id": "h2_NakDlMQup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the sentiment of new text inputs\n",
        "new_texts = [\n",
        "    \"I really enjoyed the movie\",\n",
        "    \"This product is terrible\"\n",
        "]\n",
        "predictions = pipeline(new_texts)\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtA0Pri4MNQG",
        "outputId": "63882821-b326-4b21-b41e-4d2c4e6b1e07"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [{'label': 'LABEL_1', 'score': 0.5843372344970703}, {'label': 'LABEL_1', 'score': 0.5557489395141602}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is just a simple example, and there are many ways to fine-tune transformer models with Hugging Face. However, this should give you an idea of how to use Hugging Face to perform custom sentiment analysis."
      ],
      "metadata": {
        "id": "OLOpR1P8Mfj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS-eV-vDP0_ZcP9GxCEzJFBzAoffWM8zVlwQw&usqp=CAU)"
      ],
      "metadata": {
        "id": "2pk69juNMGBG"
      }
    }
  ]
}